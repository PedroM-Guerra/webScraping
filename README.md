## üìÑ README.md (Revisado e Limpo)

# üì∞ WebScraping de Not√≠cias para Data Warehouse (ETL)

Este projeto de laborat√≥rio foca na aplica√ß√£o pr√°tica de **Web Scraping**, na constru√ß√£o de um processo de **ETL (Extract, Transform, Load)** e na modelagem e armazenamento de dados em um **Data Warehouse (DW)** utilizando **PostgreSQL**.

O objetivo √© coletar dados de not√≠cias do portal **Globo.com**, estrutur√°-los em dimens√µes (temporais, categ√≥ricas e de ve√≠culos) e, finalmente, gerar **visualiza√ß√µes de an√°lise de dados**.

-----

## üöÄ Funcionalidades e Estrutura dos Scripts

O projeto √© dividido em tr√™s etapas principais, gerenciadas por scripts Python:

### 1\. Cria√ß√£o do Banco de Dados (`dataBaseCreate.py`)

Define a estrutura do **Data Warehouse** (esquema Estrela), incluindo as tabelas Fato e Dimens√µes (**Tempo, Not√≠cia, Categoria, Ve√≠culo**).

### 2\. Scraping e ETL (`scrapingGlobo.py` e `dataBaseInsert.py`)

  * **Extra√ß√£o (E):** Coleta t√≠tulos, URLs, datas e metadados da home do portal Globo.com.
  * **Transforma√ß√£o (T):** Realiza a limpeza de texto e calcula as dimens√µes temporais (dia, m√™s, trimestre, etc.).
  * **Carregamento (L):** Insere os dados nas dimens√µes e na tabela **`Fato_Noticias`** do DW.

### 3\. Visualiza√ß√£o de Dados (`generateGraphs.py`)

Utiliza os dados consolidados no DW (lidos em um √∫nico **DataFrame** Pandas) para gerar diversos gr√°ficos anal√≠ticos, incluindo tend√™ncias por categoria, distribui√ß√£o por ve√≠culo e Nuvem de Palavras.

-----

## üì¶ Estrutura do Data Warehouse

O DW utiliza o **esquema Estrela** com a seguinte modelagem:

| Tabela | Conte√∫do Principal |
| :--- | :--- |
| **`Fato_Noticias`** | Contagem de not√≠cias (fatos) e chaves estrangeiras das dimens√µes. |
| **`Dim_Tempo`** | Detalhes temporais (ano, m√™s, dia, semana, trimestre, etc.). |
| **`Dim_Noticia`** | Metadados da not√≠cia (t√≠tulo, URL, data/hora original). |
| **`Dim_Categoria`** | Nome da categoria (e.g., 'mundo', 'politica'). |
| **`Dim_Veiculo`** | Informa√ß√µes sobre o site/ve√≠culo (nome, URL). |

-----

## üõ†Ô∏è Tecnologias Utilizadas

  * **Linguagem:** Python
  * **Banco de Dados:** PostgreSQL
  * **Web Scraping:** `requests`, `BeautifulSoup`
  * **ETL/Acesso DB:** `psycopg2`, `pandas`
  * **Visualiza√ß√£o:** `matplotlib`, `seaborn`, `wordcloud`

-----

## ‚öôÔ∏è Como Executar o Projeto

Siga os passos abaixo para configurar e rodar o projeto localmente.

### 1\. Pr√©-requisitos

1.  **PostgreSQL:** O servi√ßo deve estar rodando localmente (assumindo `localhost:5432`).
2.  **Credenciais:** As credenciais de acesso ao DB est√£o configuradas nos scripts (DB: `postgres`, User: `postgres`, Pass: `admin123`).
3.  **Ambiente Python:** Instale as bibliotecas necess√°rias:
    ```bash
    pip install psycopg2-binary requests beautifulsoup4 pandas matplotlib seaborn wordcloud
    ```

### 2\. Configurar o Banco de Dados

Execute o script de cria√ß√£o das tabelas para configurar a estrutura do DW:

```bash
python dataBaseCreate.py
```

### 3\. Rodar o Web Scraping e o ETL

Execute o script principal de scraping para coletar as not√≠cias e carreg√°-las no DW:

```bash
python scrapingGlobo.py
```

### 4\. Gerar os Gr√°ficos de An√°lise

Com os dados populados, execute o script de visualiza√ß√£o:

```bash
python generateGraphs.py
```

Este script ler√° todos os dados do DW e exibir√° os 6 gr√°ficos anal√≠ticos na tela.
